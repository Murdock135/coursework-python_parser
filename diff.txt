diff --git a/.gitignore b/.gitignore
index 6a9dd80..dd48265 100644
--- a/.gitignore
+++ b/.gitignore
@@ -18,4 +18,7 @@ wheels/
 
 # OS files
 .DS_Store
-Thumbs.db
\ No newline at end of file
+Thumbs.db
+
+generated/
+output/
\ No newline at end of file
diff --git a/IndentLexer.py b/IndentLexer.py
deleted file mode 100644
index ed17a82..0000000
--- a/IndentLexer.py
+++ /dev/null
@@ -1,152 +0,0 @@
-from typing import TextIO, Any
-import sys
-
-from minipythonLexer import minipythonLexer
-from minipythonParser import minipythonParser
-from antlr4.Token import CommonToken
-
-# ================================================================================================================
-# IndentLexer extends the ANTLR-generated minipythonLexer to add support for Python-style indentation.
-# It tracks indentation levels and emits INDENT and DEDENT tokens when the indentation changes,
-# allowing the parser to recognize block structure based on leading whitespace, similar to Python's syntax.
-
-class IndentLexer(minipythonLexer):
-    '''
-    A lexer that handles indentation-based block structure similar to Python.
-    '''
-    INDENT = minipythonParser.INDENT
-    DEDENT = minipythonParser.DEDENT
-
-    def __init__(self, input=None, output: TextIO | Any = sys.stdout):
-        super().__init__(input, output)
-
-        self.indent_stack = [0]  # Stack to hold current indentation level
-        self.pending_tokens = []  # Queue to hold pending tokens after indentation changes
-
-    def _count_indent(self) -> int:
-        '''Count the number of spaces after matching a NEWLINE token (\n)'''
-        count = 0
-        offset = 1 # Current token is \n so start checking from next token
-
-        while True:
-            char = self._input.LA(offset) # _input is an instance of InputStream. Grandparent class Lexer has self._input
-            if char == ord(' '):
-                count += 1
-            elif char == ord('\t'):
-                count += 4  # Assuming a tab is equivalent to 4 spaces
-            else:
-                break
-            offset += 1
-        
-        return count
-
-    # =================================================================================
-    # Conceptual:
-    # Take the following example:
-    # --------------------------------
-    # if x:
-    #   if y:
-    #       a = 1
-    # b = 2
-    # --------------------------------
-    # The lexer processes each line and tracks indentation:
-    # - After 'if x:\n', the lexer sees the indent increase from 0 to 4 spaces.
-    #   It pushes 4 to the indent stack, queues INDENT, and returns NEWLINE.
-    #   Stack: [0, 4]
-    # - After 'if y:\n', the indent increases from 4 to 8 spaces.
-    #   It pushes 8 to the stack, queues INDENT, and returns NEWLINE.
-    #   Stack: [0, 4, 8]
-    # - After 'a = 1\n', the indent decreases from 8 to 4 spaces.
-    #   It pops 8 from the stack, queues DEDENT, and returns NEWLINE.
-    #   Stack: [0, 4]
-    # - After 'b = 2\n', the indent decreases from 4 to 0 spaces.
-    #   It pops 4 from the stack, queues DEDENT, and returns NEWLINE.
-    #   Stack: [0]
-    # This mechanism allows the parser to recognize when blocks start and end,
-    # mirroring Python's indentation-based syntax.
-
-    def nextToken(self):
-        '''
-        Returns the next token, handling Python-style indentation.
-
-        Conceptual flow using this example:
-        -----------------------------------
-        if x:
-            if y:
-                a = 1
-        b = 2
-
-        The lexer processes each line and tracks indentation:
-        - After 'if x:\\n', the lexer sees the indent increase from 0 to 4 spaces.
-          It pushes 4 to the indent stack, queues INDENT, and returns NEWLINE.
-          Stack: [0, 4]
-        - After 'if y:\\n', the indent increases from 4 to 8 spaces.
-          It pushes 8 to the stack, queues INDENT, and returns NEWLINE.
-          Stack: [0, 4, 8]
-        - After 'a = 1\\n', the indent decreases from 8 to 4 spaces.
-          It pops 8 from the stack, queues DEDENT, and returns NEWLINE.
-          Stack: [0, 4]
-        - After 'b = 2\\n', the indent decreases from 4 to 0 spaces.
-          It pops 4 from the stack, queues DEDENT, and returns NEWLINE.
-          Stack: [0]
-
-        This mechanism allows the parser to recognize when blocks start and end,
-        mirroring Python's indentation-based syntax.
-        '''
-        # Return any pending INDENT/DEDENT tokens first
-        if self.pending_tokens:
-            print(f"Returning pending token: {[t.type for t in self.pending_tokens]}")
-            token = self.pending_tokens.pop(0)
-            return token        
-        
-        token = super().nextToken()
-        print(f"[TOKEN] type={token.type}, text={repr(token.text)}")
-
-        # If the token is a NEWLINE, check for indentation changes
-        if token.type == self.NEWLINE:
-            current_indent_level = self.indent_stack[-1]
-            next_indent_level = self._count_indent()
-            print(f"[NEWLINE] text={repr(token.text)}, current_indent={current_indent_level}, next_indent={next_indent_level}, stack={self.indent_stack}")
-
-            # Indentation detected
-            if next_indent_level > current_indent_level:
-                self.indent_stack.append(next_indent_level)
-                indent_token = CommonToken(type=self.INDENT)
-                self.pending_tokens.append(indent_token)
-
-                print(f"[INDENT] New indent level: {next_indent_level}, stack={self.indent_stack}")
-                print(f"Tokens pending: {[t.type for t in self.pending_tokens]}")
-            
-            # Dedentation(s) detected
-            elif next_indent_level < current_indent_level:
-                while next_indent_level != self.indent_stack[-1]:
-                    dedent_token = CommonToken(type=self.DEDENT)
-                    self.pending_tokens.append(dedent_token)
-                    self.indent_stack.pop()
-
-                print(f"[DEDENT] New indent level: {next_indent_level}, stack={self.indent_stack}")
-                print(f"Tokens pending: {[t.type for t in self.pending_tokens]}")
-            
-            else:
-                # Print other tokens
-                token_text = token.text if len(token.text) < 20 else token.text[:20] + "..."
-                print(f"[TOKEN] type={token.type}, text={repr(token_text)}")
-
-        # Handle EOF to emit remaining DEDENT tokens
-        elif token.type == token.EOF:
-            # At EOF, emit DEDENT tokens for any remaining indentation levels
-            while len(self.indent_stack) > 1:
-                dedent_token = CommonToken(type=self.DEDENT)
-                self.pending_tokens.append(dedent_token)
-                self.indent_stack.pop()
-            
-            print(f"[EOF] Emitting DEDENTs for remaining indent levels, stack={self.indent_stack}")
-            print(f"Tokens pending: {[t.type for t in self.pending_tokens]}")
-
-            # If there are pending tokens, return the first one
-            if self.pending_tokens:
-                token = self.pending_tokens.pop(0)
-
-        return token
-
-
diff --git a/build_and_test.sh b/build_and_test.sh
deleted file mode 100755
index 7cf286d..0000000
--- a/build_and_test.sh
+++ /dev/null
@@ -1,29 +0,0 @@
-#!/bin/bash
-set -uo pipefail
-
-# Generate the parser and run the sample program
-echo "===================================================================================="
-echo "[RUN] Starting parser generation"
-antlr4 -Dlanguage=Python3 minipython.g4
-echo "[RUN] Parser generated."
-
-echo "===================================================================================="
-echo "[RUN] Testing against python_samples/project_deliverable_1.py"
-uv run main.py python_samples/project_deliverable_1.py 2>&1 | tee output_pd1.log
-echo "[RUN] Done testing against python_samples/project_deliverable_1.py"
-
-echo "===================================================================================="
-echo "[RUN] Testing against python_samples/project_deliverable_2.py"
-uv run main.py python_samples/project_deliverable_2.py 2>&1 | tee output_pd2.log
-echo "[RUN] Done testing against python_samples/project_deliverable_2.py"
-
-# Generate the parse tree
-echo "===================================================================================="
-echo "[RUN] Generating parse trees"
-antlr4-parse minipython.g4 prog -gui python_samples/project_deliverable_1.py > parse_tree_1.ps 2>> run.log
-echo "[RUN] Parse tree for project_deliverable_1.py generated as parse_tree_1.ps"
-
-echo "===================================================================================="
-echo "[RUN] Generating parse tree for project_deliverable_2.py"
-antlr4-parse minipython.g4 prog -gui python_samples/project_deliverable_2.py > parse_tree_2.ps 2>> run.log
-echo "[RUN] Parse tree for project_deliverable_2.py generated as parse_tree_2.ps"
\ No newline at end of file
diff --git a/main.py b/main.py
deleted file mode 100644
index 6003da7..0000000
--- a/main.py
+++ /dev/null
@@ -1,34 +0,0 @@
-import sys
-from antlr4 import FileStream, CommonTokenStream, ParseTreeWalker
-from IndentLexer import IndentLexer
-from minipythonParser import minipythonParser
-from minipythonListener import minipythonListener
-
-def main():
-    # Run python file. Use the parser and lexer generated by ANTLR to parse a sample Python file.
-
-    # Check for input file argument
-    if len(sys.argv) < 2:
-        print("Usage: python main.py <input_file>")
-        sys.exit(1)
-
-    input_file = sys.argv[1]
-    print(f"Parsing file: {input_file}")
-
-    # create input stream -> lexer -> token stream -> parser
-    input_stream = FileStream(input_file)
-    lexer = IndentLexer(input_stream)
-    tokens = CommonTokenStream(lexer)
-    parser = minipythonParser(tokens)
-
-    # parse the input file starting from the 'prog' rule
-    tree = parser.prog()
-
-    # walk the parse tree and print the nodes
-    walker = ParseTreeWalker()
-    printer = minipythonListener()
-    walker.walk(printer, tree)
-
-
-if __name__ == '__main__':
-    main()
\ No newline at end of file
diff --git a/minipython.g4 b/minipython.g4
deleted file mode 100644
index 0c005a8..0000000
--- a/minipython.g4
+++ /dev/null
@@ -1,101 +0,0 @@
-grammar minipython;
-
-// ==============================================================================
-// Parser rules (rules that define the structure of the language)
-// ==============================================================================
-
-tokens {INDENT, DEDENT}
-
-prog: block EOF;
-
-block: (statement NEWLINE+)* statement?;
-
-statement:
-	| assignment
-	| compound_assignment
-	| if_stmt
-	| expr
-	;
-
-assignment:
-	ID '=' expr
-	;
-
-compound_assignment:
-	ID COMPOUND_OP expr
-	;
-
-// *************************************************************************************************
-// QUESTION FOR EKIN: Why can't we use 'if' expr ':' NEWLINE INDENT block DEDENT ... directly here?
-// *************************************************************************************************
-if_stmt:
-	IF expr ':' NEWLINE INDENT block DEDENT
-	(ELIF expr ':' NEWLINE INDENT block DEDENT)*
-	(ELSE ':' NEWLINE INDENT block DEDENT)?
-	;
-
-expr:
-	| expr OP_1 expr // multiplicative
-	| expr OP_2 expr // additive
-	| expr OP_3 expr // comparison
-	| expr AND expr // logical AND
-	| expr OR expr // logical OR
-	| NOT expr // logical NOT
-	| '(' expr ')' // parenthesized expr
-	| '(' expr ',' expr (',' expr)* ')' // tuple (2+ elements)
-	| '(' expr ',' ')' // single-element tuple
-    | '[' expr (',' expr)* ']' // list
-    | '{' expr ':' ( expr (',' expr ':' expr)* )? '}' // dict
-	| atom;
-
-atom: NUMBER | ID | STRING | BOOL;
-
-// TODO:
-// -----------------------------------------------------------------------------
-// 1. if statements
-// 2. for loops
-// 3. while loops
-
-// ==============================================================================
-// Lexer rules (tokens)
-// ==============================================================================
-
-// -------------------------------------------------------------------------------------
-// The following are defined BEFORE ID
-IF: 'if';
-ELIF: 'elif';
-ELSE: 'else';
-AND: 'and';
-OR: 'or';
-NOT: 'not';
-
-NUMBER: INT | FLOAT;
-FLOAT: '-'? [0-9]+ '.' [0-9]+; // Floating point literals
-INT: '-'? [0-9]+; // Integer literals
-BOOL: 'True' | 'False'; // Boolean literals
-STRING:
-	'"' ('\\' . | ~["\\\r\n])* '"' // Double-quoted strings: Backslash escapes followed by any character OR any character except backslash, double-quote, carriage return, or newline
-	| '\'' ( '\\' . | ~['\\\r\n])* '\''; // Single-quoted strings: Backslash escapes followed by any character OR any character except backslash, single-quote, carriage return, or newline
-
-// Operators
-OP_1: '*' | '/' | '%'; // Multiplicative operators
-OP_2: '+' | '-'; // Additive operators
-OP_3:
-	'=='
-	| '!='
-	| '<'
-	| '<='
-	| '>'
-	| '>='; // Comparison operators
-COMPOUND_OP: (OP_1 | OP_2) '='; // Compound assignment operators
-
-// *************************************************************************************************************************
-// QUESTION FOR EKIN: Why can't we use this? Claude mentioned that we canot reference other token names in Lexer rules
-// LOGICAL_OP: AND | OR | NOT; // Logical operators
-// **************************************************************************************************************************
-
-//----------------------------------------------------------------------------------------
-ID: [a-zA-Z_][a-zA-Z0-9_]*; // Identifiers start with letters or underscore
-COMMENT: '#' ~[\r\n]* -> skip; // Skip comments
-NEWLINE: '\r\n' | '\n' | '\r'; // Newline characters
-WS: [ \t]+ -> skip; // Skip spaces, tabs and newlines
\ No newline at end of file
diff --git a/python_samples/project_deliverable_1.py b/python_samples/project_deliverable_1.py
deleted file mode 100644
index 549c856..0000000
--- a/python_samples/project_deliverable_1.py
+++ /dev/null
@@ -1,27 +0,0 @@
-assign1 = "20"
-assign2 = 123
-assign3 = 1.23
-assign4 = assign1
-
-arith_op1 = 1 + 2
-arith_op2 = 13 - 3
-arith_op3 = 10 / arith_op1
-arith_op4 = 4.2 * 10
-arith_op5 = arith_op1 % arith_op2
-
-arith_op1 += arith_op2
-arith_op2 -= arith_op3
-arith_op3 *= arith_op4
-arith_op4 /= arith_op5
-
-array1 = [1, 2, 3, 4, 5]
-array2 = ['a', 'b', 'c']
-array3 = [1.6, 2.7, 3.8, 4.9, 5.0]
-
-
-var1 = 10
-var2 = var1/2 + 5
-var3 = var2 % 2
-var4 = 1
-
-flag = True
\ No newline at end of file
diff --git a/python_samples/project_deliverable_2.py b/python_samples/project_deliverable_2.py
deleted file mode 100644
index 51f830e..0000000
--- a/python_samples/project_deliverable_2.py
+++ /dev/null
@@ -1,48 +0,0 @@
-assign1 = "20"
-assign2 = 123
-assign3 = 1.23
-assign4 = assign1
-
-arith_op1 = 1 + 2
-arith_op2 = 13 - 3
-arith_op3 = 10 / arith_op1
-arith_op4 = 4.2 * 10
-arith_op5 = arith_op1 % arith_op2
-
-arith_op1 += arith_op2
-arith_op2 -= arith_op3
-arith_op3 *= arith_op4
-arith_op4 /= arith_op5
-
-array1 = [1, 2, 3, 4, 5]
-array2 = ['a', 'b', 'c']
-array3 = [1.6, 2.7, 3.8, 4.9, 5.0]
-
-var1 = 10
-var2 = var1/2 + 5
-var3 = var2 % 2
-var4 = 1
-
-flag = True
-
-assign1 = ""
-
-if var1 > var2:
-	arith_op1 = 1 + 2
-	assign1 = "text data"
-
-if var1 <= var2 and var3 == var4:
-	arith_op1 = 1 + 2
-	assign1 = "text data"
-else:
-	arith_op4 = 4.2 * 10
-	arith_op3 *= arith_op4
-
-data = 0
-
-if var1 != var2 or var3 >= var4:
-	flag = True
-elif (not flag) and var3 > 10:
-	flag = False
-else:
-	data = -1
\ No newline at end of file
diff --git a/run.sh b/run.sh
deleted file mode 100644
index 118a777..0000000
--- a/run.sh
+++ /dev/null
@@ -1,3 +0,0 @@
-#!/bin/bash
-
-bash build_and_test.sh | tee run.log
\ No newline at end of file
diff --git a/todo.md b/todo.md
index 5c683a0..6dc3e2d 100644
--- a/todo.md
+++ b/todo.md
@@ -10,6 +10,9 @@
 7. Move bash scripts to scripts/
     - build_and_test.sh
     - run.sh
+8. Update bash scripts
+    - build_and_test.sh
+    - run.sh
 
 8. Update .gitignore
     - add generated/
